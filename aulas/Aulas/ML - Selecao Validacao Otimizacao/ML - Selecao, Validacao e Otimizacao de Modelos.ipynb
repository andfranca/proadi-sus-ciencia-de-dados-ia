{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Seleção, Validação e Otimizacao de Modelos**\n",
        "**Autor:** [Anderson França](https://www.linkedin.com/in/anderson-m-franca/) | **Contato:** [github.com/andfranca](https://github.com/andfranca/estatistica-e-aprendizado-de-maquinas-ptbr)\n",
        "\n",
        "<a href=\"https://creativecommons.org/licenses/by/4.0/deed.en\"><img align=\"left\" width=\"80\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc.png\"/></a>\n"
      ],
      "metadata": {
        "id": "XMknkSsbqTOH"
      },
      "id": "XMknkSsbqTOH"
    },
    {
      "cell_type": "markdown",
      "id": "a1cc50ee",
      "metadata": {
        "id": "a1cc50ee"
      },
      "source": [
        "## **Avaliação de Modelos**\n",
        "\n",
        "Avaliar um modelo é essencial para garantir que suas previsões sejam confiáveis e úteis.\n",
        "\n",
        "Um erro comum é medir o desempenho apenas nos dados de treino, o que pode gerar uma falsa sensação de qualidade.\n",
        "\n",
        "O objetivo da avaliação é testar a capacidade do modelo de generalizar para novos dados. Como analisar previsão por previsão seria inviável, usamos métricas que resumem o desempenho em um único valor.\n",
        "\n",
        "**Principais métricas:**\n",
        "- **Classificação:** Acurácia, Precision, Recall, F1-Score, AUC-ROC\n",
        "- **Regressão:** R², RMSE, MAE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fluxo de Treinamento de Modelos**\n",
        "\n",
        "O fluxo de treinamento é o processo estruturado onde um modelo de machine learning aprende padrões a partir dos dados disponíveis, ajustando seus parâmetros para minimizar erros e gerar boas previsões.\n",
        "\n",
        "Esse fluxo inclui desde o preparo dos dados até a avaliação inicial do modelo, garantindo que ele esteja pronto para ser validado e aplicado em novos dados.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1MGKpA-vzEY6D7VUKx3FyeDp9FHsgGjXk\" width=\"600\" align=\"center\"/>\n",
        "    \n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "p7cfEywNsYIM"
      },
      "id": "p7cfEywNsYIM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Trade-off entre Viés e Variância**\n",
        "\n",
        "\n",
        "Ao treinar um modelo de Machine Learning, nosso objetivo é que ele **aprenda padrões reais** dos dados e consiga fazer previsões precisas em novos cenários.\n",
        "\n",
        "Porém, existe um equilíbrio delicado:\n",
        "- Se o modelo for simples demais, ele não aprende direito: alto viés (bias).\n",
        "- Se for complexo demais, ele pode aprender até o ruído: alta variância.\n",
        "\n",
        "Esse equilíbrio é chamado de **Trade-off Viés (bias) x Variância**, e vamos falar um pouco sobre esses conceitos.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1nKdsrd4GKvi74Yc7BImNd8Xvhkfv0ynz\" width=\"600\" align=\"center\"/>\n",
        "    \n",
        "</div>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t_NzIJe8shTh"
      },
      "id": "t_NzIJe8shTh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Bias**\n",
        "\n",
        "O Bias (Viés) representa a tendência de um modelo **simplificar demais o problema**.\n",
        "\n",
        "Quando um modelo tem alto viés, ele assume suposições muito rígidas e acaba **não capturando bem os padrões dos dados**.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1HPe3QpJ854gQnMhzFIhBVHjMWqqDAXpY\" width=\"300\" align=\"center\"/>\n",
        "    \n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "**Consequência:**\n",
        "\n",
        "O modelo tem desempenho ruim tanto nos dados de treino quanto em dados novos: Underfitting."
      ],
      "metadata": {
        "id": "iOI3hoYlshKf"
      },
      "id": "iOI3hoYlshKf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Variância**\n",
        "\n",
        "**Variância** mede o quanto o modelo é **sensível às variações nos dados de treino.**\n",
        "\n",
        "Quando um modelo tem alta variância, ele se ajusta demais aos dados disponíveis, capturando inclusive o ruído e as peculiaridades dos dados de treino.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1s6G5NpRPfeniGUMXWZUhETtz6zqH3k5l\" width=\"300\" align=\"center\"/>\n",
        "    \n",
        "</div>\n",
        "\n",
        "**Consequência:**\n",
        "O modelo tem ótimo desempenho no treino, mas desempenho ruim em novos dados: **Overfitting.**"
      ],
      "metadata": {
        "id": "qRwTiST4shCm"
      },
      "id": "qRwTiST4shCm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Trade-Off: Baixo Bias e Baixa Variância**\n",
        "\n",
        "O desafio em Machine Learning é encontrar um modelo que tenha bias e variância equilibrados.\n",
        "- **Baixo Bias:** Modelo captura bem os padrões dos dados (evita underfitting).\n",
        "- **Baixa Variância:** Modelo não se ajusta demais ao treino, generaliza bem (evita overfitting).\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1jQCEUk60ASVJ350wTD9V_Pw3JMUalVz7\" width=\"300\" align=\"center\"/>\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "0OQZ4G-Asg6H"
      },
      "id": "0OQZ4G-Asg6H"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Erro Irredutível**\n",
        "\n",
        "O **Erro Irredutível** é aquela parte do erro que **não pode ser eliminada**, independentemente do algoritmo utilizado.\n",
        "\n",
        "Ele não acontece porque o modelo é incapaz, mas sim por fatores como:\n",
        "\n",
        "- Variáveis desconhecidas ou não observadas nos dados.\n",
        "- Ruídos, erros de medição ou registros imprecisos.\n",
        "\n",
        "Embora não possamos eliminar totalmente, podemos **minimizar** esse erro com:\n",
        "- Melhor coleta de dados\n",
        "- Limpeza e padronização adequada dos dados\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1nKdsrd4GKvi74Yc7BImNd8Xvhkfv0ynz\" width=\"300\" align=\"center\"/>\n",
        "    \n",
        "</div>\n",
        "\n",
        "\n",
        "$\\text{Erro} = \\text{Bias}^2 + \\text{Variância} + \\text{Erro Irredutível}$"
      ],
      "metadata": {
        "id": "w8H7AjJGsg2S"
      },
      "id": "w8H7AjJGsg2S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Como Identificar no Modelo?\n",
        "\n",
        "Entender conceitos como bias, variância e erro total é essencial — mas tão importante quanto isso é saber identificar na prática qual problema o modelo apresenta.\n",
        "\n",
        "A partir da análise dos erros no conjunto de treino e teste, conseguimos diagnosticar se o modelo está sofrendo de:\n",
        "\n",
        "| **Sinais durante avaliação**                     | **Indicação**                             |\n",
        "|---------------------------------------------|----------------------------------------|\n",
        "| Erro alto no treino e no teste              | Modelo com Alto Bias (Underfitting)    |\n",
        "| Erro baixo no treino, mas alto no teste     | Modelo com Alta Variância (Overfitting)|\n",
        "| Erro baixo e consistente nos dois           | Modelo equilibrado (bom generalizador) |"
      ],
      "metadata": {
        "id": "zeJjKGV5sgtL"
      },
      "id": "zeJjKGV5sgtL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mHkSloblzn0i"
      },
      "id": "mHkSloblzn0i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Métricas de Avaliação de Modelos**\n",
        "\n",
        "Escolher as métricas corretas é essencial para avaliar, de forma confiável, a performance do modelo. Aqui estão alguns critérios práticos:\n",
        "\n",
        "\n",
        ">  * **Tipo de Problema:**\n",
        "Use métricas adequadas ao tipo de tarefa: classificação ou regressão.\n",
        "\n",
        "\n",
        "> * **Balanceamento dos Dados:**\n",
        "Em dados desbalanceados, priorize métricas como **Precision, Recall, F1-Score.**\n",
        "\n",
        "\n",
        "> * **Objetivo do Problema:**\n",
        "Escolha métricas alinhadas ao impacto prático (ex.: **Recall** para evitar falsos negativos em saúde pública).\n",
        "\n",
        "\n",
        "> * **Simplicidade e Interpretabilidade:**\n",
        "Prefira métricas que ajudem na manutenção e explicação do modelo.\n",
        "\n",
        "\n",
        "> * **Comparabilidade:**\n",
        "Use métricas reconhecidas pelo setor, para garantir avaliações consistentes.\n",
        "\n",
        "\n",
        ">*  **Monitoramento ao Longo do Tempo:**\n",
        "Adote métricas que permitam acompanhar estabilidade e performance do modelo no tempo.\n"
      ],
      "metadata": {
        "id": "z143dLYQsgq5"
      },
      "id": "z143dLYQsgq5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "x-1M-aRezthV"
      },
      "id": "x-1M-aRezthV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Métricas para Regressão**\n",
        "\n",
        "Para garantir que um modelo funciona, é preciso testar o resultado desse modelo. A **avaliação do modelo** de regressão linear é essencial para entender o quão bem ele se ajusta aos dados e sua capacidade de fazer previsões em novos dados.\n",
        "\n",
        "Ao prever um valor numérico como valor monetário, distância ou altura, o objetivo não é saber se o modelo previu o valor exatamente, e sim o quão próximas as precisões estavam dos valores esperados.\n",
        "\n",
        "Existem diversas métricas de erro que são usadas com frequência para avaliar e relatar o desempenho de um modelo de regressão.\n",
        "\n",
        "* R²\n",
        "* Erro Absoluto Médio\n",
        "* Erro Quadrático Médio\n",
        "* Raiz do Erro Quadrático Médio\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1JwgJwChg1aKfWPXK5SM-rrQfcNFM7E4o\" width=\"600\" align=\"center\"/>\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "gMpxNENvsgn4"
      },
      "id": "gMpxNENvsgn4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Carregando a base de dados para avaliar métricas de regressão"
      ],
      "metadata": {
        "id": "DXAJSkFSN8oN"
      },
      "id": "DXAJSkFSN8oN"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "energia = pd.read_csv('https://raw.githubusercontent.com/andfranca/consumo-energia/main/Base%20de%20Dados/Previs%C3%A3o%20Energia%20Solar/previsao_energia_solar.csv', decimal = ',')\n",
        "\n",
        "#Definir as variáveis que serão utilizadas no modelo\n",
        "y = pd.DataFrame(energia['Geracao'])\n",
        "X = pd.DataFrame(energia[['Localizacao'\n",
        "                          , 'Data'\n",
        "                          , 'Mes'\n",
        "                          , 'Hora'\n",
        "                          , 'Estacao'\n",
        "                          , 'Umidade'\n",
        "                          , 'Temperatura'\n",
        "                          , 'Vento'\n",
        "                          , 'Visibilidade'\n",
        "                          , 'Pressao'\n",
        "                          , 'Cloud.Ceiling'\n",
        "                          ]])\n",
        "\n",
        "\n",
        "# Carregar função train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dividir os dados em 80% treino e 20% teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2, #0.2 corresponde a 20%\n",
        "                                                    random_state=42)\n",
        "\n",
        "\n",
        "# Treinar o modelo de regressão linear com a base de treino\n",
        "modelo = LinearRegression()\n",
        "modelo.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões com os dados de teste\n",
        "y_pred = modelo.predict(X_test)\n"
      ],
      "metadata": {
        "id": "SlgKfZA9OCrO"
      },
      "id": "SlgKfZA9OCrO",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### R2 score\n",
        "\n",
        "O _R2 Score_ Representa a proporção da variância (de y) que foi explicada pelas variáveis independentes no modelo. Ele fornece uma indicação da qualidade do ajuste e, portanto, uma medida de quão bem as amostras não vistas provavelmente serão previstas pelo modelo, por meio da proporção da variância explicada.\n",
        "\n",
        "Como essa variação depende do conjunto de dados, os $R^2$ podem não ser significativamente comparáveis em diferentes conjuntos de dados. O melhor _score_ possível é 1,0 e pode ser negativa (porque o modelo pode ser arbitrariamente pior).\n",
        "\n",
        "Se $\\hat{y}_i$ for o valor previsto da $i$-ésima amostra e $y_i$ for o valor verdadeiro correspondente para o total de $n$ amostras, o $R^2$ estimado é definido como:\n",
        "\n",
        "$$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$\n",
        "\n",
        "Fonte: [Scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)"
      ],
      "metadata": {
        "id": "-GTvcJktNVfo"
      },
      "id": "-GTvcJktNVfo"
    },
    {
      "cell_type": "code",
      "source": [
        "r2 = metrics.r2_score(y_test, y_pred)\n",
        "r2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NR22N9DOscM",
        "outputId": "f5254f16-2bed-4bb1-f4e1-0805df1135f5"
      },
      "id": "0NR22N9DOscM",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4646805223318039"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### R2 score Ajustado\n",
        "A desvantagem do R2 score é que, ao adicionar novos recursos nos dados, o score começa a aumentar ou permanece constante, mas nunca diminui porque assume que, ao adicionar mais dados, a variação dos dados aumenta.\n",
        "\n",
        "Mas o problema é quando adicionamos um recurso irrelevante no conjunto de dados e, nesse momento, o R2 às vezes começa a aumentar, o que é incorreto. Uma forma de lidar com esse tipo de problema, é utilizar o R2 ajustado.\n",
        "\n",
        "<center>${\\displaystyle {\\bar {R}}^{2}=1-(1-R^{2}){n-1 \\over n-p}}$"
      ],
      "metadata": {
        "id": "IicZIRatNcpw"
      },
      "id": "IicZIRatNcpw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Onde:\n",
        "- $n$, é o número de observações na base\n",
        "- $p$, é o número de variáveis independentes\n",
        "- $R^2$, é o coeficiente de determinação"
      ],
      "metadata": {
        "id": "5Zu-KJ7NNi1k"
      },
      "id": "5Zu-KJ7NNi1k"
    },
    {
      "cell_type": "code",
      "source": [
        "n=X_test.shape[0] #qtde observações\n",
        "k=X_test.shape[1] #qtde de variáveis\n",
        "adj_r2_score = 1 - ((1-r2)*(n-1)/(n-k-1))\n",
        "adj_r2_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2OnNneAOvfk",
        "outputId": "4b81bb6f-caa8-4faf-a177-daa3c39dd87c"
      },
      "id": "T2OnNneAOvfk",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46327749296455345"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Absolute Error - MAE\n",
        "\n",
        "O erro absoluto médio (MAE) é uma das métricas mais comuns usadas para calcular o erro de previsão do modelo. O erro de previsão de uma única linha de dados é:\n",
        "\n",
        "$ErroPrevisao = ValorAtual - ValorPrevisto$\n",
        "\n",
        "Precisamos calcular os erros de previsão para cada linha de dados, obter seu valor absoluto e, em seguida, encontrar a média de todos os erros de previsão absolutos.\n",
        "\n",
        "O MAE é dado pela seguinte fórmula:\n",
        "\n",
        "\n",
        "<center> $\\text{MAE}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\left| y_i - \\hat{y}_i \\right|$</center>\n",
        "\n",
        "\n",
        "\n",
        "O gráfico abaixo representa os resíduos – diferenças entre os valores previstos (linha de regressão) e os valores de saída. O MAE usa o valor absoluto dos resíduos, portanto, não pode indicar se o modelo está com desempenho inferior ou superior.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://i.ibb.co/6m8j6Bz/MAEgraphical.png\" width=\"300\" align=\"left\"/>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "3JVo3rx5Nmbb"
      },
      "id": "3JVo3rx5Nmbb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonte: [Métricas de regressão](https://www.datacourses.com/evaluation-of-regression-models-in-scikit-learn-846/)\n",
        "\n",
        "Por esse motivo, um MAE pequeno sugere que o modelo é ótimo em previsão. Da mesma forma, um grande MAE sugere que seu modelo pode ter problemas para generalizar bem.\n",
        "\n",
        "Uma das grandes vantagens de se utilizar o MAE é que ele está na mesma unidade da variável de saída e é bem robusto quanto aos outliers."
      ],
      "metadata": {
        "id": "GshMmK6nNpvd"
      },
      "id": "GshMmK6nNpvd"
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculando o MAE\n",
        "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
        "mae"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eDVXQBBOzNv",
        "outputId": "d0e71af7-758a-4a27-dd24-3277910a63ef"
      },
      "id": "5eDVXQBBOzNv",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.128572910028985"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean squared error - MSE\n",
        "\n",
        "O erro quadrático médio (MSE) calcula a diferença quadrática média entre os valores alvo e previsto. Esta métrica é utilizada para muitos problemas de regressão, onde os erros maiores têm contribuições quadradas correspondentemente maiores para o erro médio.\n",
        "\n",
        "O MSE é dado pela seguinte fórmula:\n",
        "\n",
        "<center>$\\text{MSE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2$</center>\n",
        "\n",
        "<div>\n",
        "<img src=\"https://i.ibb.co/QpbsgNw/MSEgraphical.png\" width=\"300\" align=\"left\"/>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "KcHZoSKjNsRR"
      },
      "id": "KcHZoSKjNsRR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "O MSE quase sempre será maior que o MAE porque no MAE os resíduos contribuem linearmente para o erro total, enquanto no MSE o erro cresce quadraticamente a cada resíduo. É por isso que o MSE é usado para determinar até que ponto o modelo se ajusta aos dados porque penaliza fortemente os outliers."
      ],
      "metadata": {
        "id": "aPwYykD1Nuuv"
      },
      "id": "aPwYykD1Nuuv"
    },
    {
      "cell_type": "code",
      "source": [
        "mse = metrics.mean_squared_error(y_test, y_pred)\n",
        "mse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlEfSrggO1-x",
        "outputId": "b29551a9-13b0-40b7-9b8f-561f05ecf398"
      },
      "id": "ZlEfSrggO1-x",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27.194501157938348"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Root Mean Squared Error - RMSE\n",
        "Root Mean Square Error (RMSE) é a raiz quadrada do MSE. É uma métrica utilizada com mais frequência do que o MSE. Muitas vezes o MSE pode ser um valor muito grande, o que pode dificultar na comparação, e, ao se utilizar o RMSE, é possível \"voltar\" os valores na mesma escala da base original, o que facilita a interpretação."
      ],
      "metadata": {
        "id": "J0jyBU0RNxJZ"
      },
      "id": "J0jyBU0RNxJZ"
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
        "rmse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqbBJR33O5jH",
        "outputId": "bdc9b471-21e1-45d3-c07c-cc46f32dd366"
      },
      "id": "aqbBJR33O5jH",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.2148347200978815"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Resumindo**\n",
        "* **R²:** Avalia a quantidade de variação explicada pelas variáveis preditivas. Um R² alto indica bom ajuste, mas não diz nada sobre o tamanho absoluto dos erros.\n",
        "* **MAE:** Fornece uma média direta de quão distantes estão as previsões dos valores reais, sem amplificar grandes erros.\n",
        "* **MSE:** Amplifica grandes erros, sendo útil quando erros grandes são mais prejudiciais.\n",
        "* **RMSE:** Traz o MSE de volta à escala original dos dados, mantendo a penalização de grandes erros.\n",
        "\n",
        "\n",
        "#### **Considerações:**\n",
        "\n",
        "* Ao escolher a métrica para a avaliação do modelo, precisamos garantir que a métrica **penalize os erros** de uma forma que reflita as consequências desses erros para as **necessidades de negócios**.\n",
        "* Se houver outliers nos dados, eles podem ter uma influência indesejada no score geral de R2 ou MSE.\n",
        "* **MAE:** mais robusto a outliers, ideal quando não queremos que grandes erros dominem a avaliação.\n",
        "* **MSE/RMSE:** sensíveis a outliers, úteis quando grandes erros precisam ser penalizados mais fortemente.\n",
        "* O **MAE** é a melhor métrica quando queremos fazer uma distinção entre diferentes modelos porque não reflete grandes resíduos.\n"
      ],
      "metadata": {
        "id": "z18Mprx0sgkx"
      },
      "id": "z18Mprx0sgkx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "MnWZB7ahzrML"
      },
      "id": "MnWZB7ahzrML"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Carregar e processar base de dados para classificação**"
      ],
      "metadata": {
        "id": "rBqB3AaD1KEc"
      },
      "id": "rBqB3AaD1KEc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando bibliotecas\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#Definir base de treinamento e teste\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/andfranca/proadi-sus-ciencia-de-dados-ia/refs/heads/main/bases/base_atrito.csv',low_memory=False)\n",
        "\n",
        "variaveis_categoricas = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "label_encoders = {}\n",
        "\n",
        "# Aplicar LabelEncoder a cada coluna categórica\n",
        "for i in variaveis_categoricas:\n",
        "    le = LabelEncoder()\n",
        "    df[i] = le.fit_transform(df[i])\n",
        "    label_encoders[i] = le\n",
        "\n",
        "\n",
        "# Definir variáveis independente e dependentes\n",
        "x = df.drop(['Deixou a empresa'], axis=1)\n",
        "y = df['Deixou a empresa']\n",
        "\n",
        "\n",
        "#Definir bases de treino e teste, com 80% para treinamento do modelo\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y,\n",
        "                                                    test_size = 0.20,\n",
        "                                                    random_state = 42)\n",
        "\n",
        "\n",
        "#Ajustar o modelor de regressão logística\n",
        "modelo_log = LogisticRegression(random_state = 0)\n",
        "modelo_log.fit(X_train, y_train)\n",
        "y_pred_log = modelo_log.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z5y97Wcg0e0s",
        "outputId": "42457ec3-df98-46f1-bc88-29ef5736116b"
      },
      "id": "z5y97Wcg0e0s",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **Métricas de Classificação**\n",
        "\n",
        "**Métricas de classificação** são indicadores usados para avaliar o desempenho de modelos em tarefas de **classificação**, onde o objetivo é prever rótulos discretos (categorias ou classes).\n",
        "\n",
        "Essas métricas ajudam a entender quão bem o modelo está prevendo as classes corretas e são essenciais para comparar modelos e otimizar seus resultados.\n",
        "\n",
        "Vamos analisar as métricas mais utilizadas:\n",
        "\n",
        "* Matriz de Confusão\n",
        "* Acurácia\n",
        "* Precision\n",
        "* Recall\n",
        "* F1-Score\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1SKPEzKPFJoBwhkL0PB5fofQ2XYq8KDp3\" width=\"600\" align=\"center\"/>\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "pncFWQB7sgdP"
      },
      "id": "pncFWQB7sgdP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Matriz de Confusão**\n",
        "\n",
        "A matriz de confusão é uma tabela que permite a visualização do desempenho de um algoritmo de classificação. Cada linha da matriz representa as instâncias em uma classe real, enquanto cada coluna representa as instâncias em uma classe prevista, ou vice-versa.\n",
        "\n",
        "\n",
        "<table style=\"background:#FFFFFF;\" align=\"center\">\n",
        "    \n",
        "<tbody><tr>\n",
        "<td rowspan=\"2\" style=\"border:none;background:#FFFFFF;\">\n",
        "</td>\n",
        "<td style=\"border:none; background:#FFFFFF;\" >\n",
        "</td>\n",
        "<td colspan=\"2\" style=\"background:#bbeeee;\"><b>Previsto</b>\n",
        "</td></tr>\n",
        "<tr>\n",
        "<td style=\"background:#ffffff;\">\n",
        "</td>\n",
        "<td style=\"background:#ccffff;\"><b>Negativo (PN)</b>\n",
        "</td>\n",
        "<td style=\"background:#ccffff;\"><b>Positivo (PP)</b>\n",
        "</td></tr>\n",
        "<tr>\n",
        "<td rowspan=\"2\" ><div ><b>Atual</b></div>\n",
        "</td>\n",
        "<td style=\"background:#ccffff;\"><b>Negativo (N)</b>\n",
        "</td>\n",
        "<td style=\"background:#ccffcc;\"><b>Verdadeiro negativo</a> (TN) <br></b>\n",
        "</td>\n",
        "<td style=\"background:#ffdddd;\"><b>Falso Positivo</a> (FP) <br></b>\n",
        "</td></tr>\n",
        "<tr>\n",
        "<td style=\"background:#ccffff;\"><b>Positivo (P)</b>\n",
        "</td>\n",
        "<td style=\"background:#ffcccc;\"><b>Falso Negativo</a> (FN) <br></b>\n",
        "</td>\n",
        "<td style=\"background:#bbeebb;\"><b>Verdadeiro positivo</a> (TP) <br></b>\n",
        "</td></tr>\n",
        "<tr>\n",
        "\n",
        "</td></tr></tbody></table>\n",
        "\n",
        "[Origem da Tabela](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n",
        "\n",
        "\n",
        "- Verdadeiros Positivos (TP)\n",
        "- Falsos Positivos (FP) - Erro Tipo I\n",
        "- Verdadeiros Negativos (TN)\n",
        "- Falsos Negativos (FN) - Erro tipo II\n",
        "\n",
        "\n",
        "Resumindo as métricas\n",
        "- TP = Era verdadeiro e o modelo previu verdadeiro\n",
        "- FP = Era falso e o modelo previu verdadeiro\n",
        "- TN = Era falso e o modelo previu falso\n",
        "- FN = Era verdadeiro e o modelo previu falso"
      ],
      "metadata": {
        "id": "IaCxhjhKz4XF"
      },
      "id": "IaCxhjhKz4XF"
    },
    {
      "cell_type": "code",
      "source": [
        "#Matriz de Confusão usando sklearn\n",
        "from sklearn import metrics\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, y_pred_log)\n",
        "print (\"Confusion Matrix : \\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLwB4XOf1gOU",
        "outputId": "713d00e0-5ceb-4aa1-c40c-27ba6df9891c"
      },
      "id": "BLwB4XOf1gOU",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix : \n",
            " [[4943 2861]\n",
            " [3248 3848]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O resultado da matriz tem:\n",
        "\n",
        "* 4.943 Verdadeiro negativo\n",
        "* 3.248 Falso Negativo\n",
        "* 2.861 Falso Positivo\n",
        "* 3.848 Verdadeiro Positivo"
      ],
      "metadata": {
        "id": "TD88VOOI1iFl"
      },
      "id": "TD88VOOI1iFl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Acurácia (ACC)**\n",
        "Indica a performance geral do modelo. Dentre todas as classificações, quantas o modelo classificou corretamente\n",
        "\n",
        "$$\n",
        "\\text{Acurácia} = \\frac{\\text{Previsões corretas}}{\\text{Total de previsões}}\n",
        "$$\n",
        "\n",
        "A acurácia é um bom indicador geral de como o modelo performou. Porém, pode haver situações em que ela é enganosa."
      ],
      "metadata": {
        "id": "sNwQnTDaz4U2"
      },
      "id": "sNwQnTDaz4U2"
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculando a acurácia com o Sklearn\n",
        "acc = metrics.accuracy_score(y_test, y_pred_log)\n",
        "print(\"Acurácia:\",acc)\n",
        "print(f\"A taxa total de acerto do modelo é {acc :.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYk0LFDk2Fi_",
        "outputId": "a97ea2c1-7dc8-4c34-f0bf-a95368a6c56d"
      },
      "id": "zYk0LFDk2Fi_",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.59\n",
            "A taxa total de acerto do modelo é 59.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "1YQ9PCK62eyJ"
      },
      "id": "1YQ9PCK62eyJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Precision (PPV)**\n",
        "\n",
        "Valor preditivo positivo  (PPV)\n",
        "\n",
        "Dentre **todas** as classificações de classe Positivo que o modelo fez, quantas estão corretas\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "\n",
        "A precisão pode ser usada em uma situação em que os **Falsos Positivos** são considerados mais prejudiciais que os Falsos Negativos."
      ],
      "metadata": {
        "id": "SteZjuDLz4Sk"
      },
      "id": "SteZjuDLz4Sk"
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculando o precision com o Sklearn\n",
        "precision = metrics.precision_score(y_test, y_pred_log)\n",
        "print (\"Precision :\", precision)\n",
        "\n",
        "print(f\"O precision indica que de todas as previsões positivas, o modelo acertou {precision :.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IM27LHB2HUt",
        "outputId": "9ff11c01-4f4c-439c-d624-3994e96f56cb"
      },
      "id": "4IM27LHB2HUt",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision : 0.5735579072887167\n",
            "O precision indica que de todas as previsões positivas, o modelo acertou 57.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "4o0c9rQH2gZO"
      },
      "id": "4o0c9rQH2gZO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Recall (TPR)**\n",
        "\n",
        "Sensibilidade ou Taxa de Verdadeiros Positivos\n",
        "\n",
        "O recall é o número de pessoas que o modelo identificou corretamente como tendo diabetes dividido pelo número total de pessoas que realmente têm a doença.\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "O recall pode ser usada em uma situação em que os **Falsos Negativos** são considerados mais prejudiciais que os Falsos Positivos."
      ],
      "metadata": {
        "id": "nyqXdE5Gz4QY"
      },
      "id": "nyqXdE5Gz4QY"
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculando o recall com o Sklearn\n",
        "recall = metrics.recall_score(y_test, y_pred_log)\n",
        "print (\"Precision :\", recall)\n",
        "\n",
        "print(f\"O recall indica que de todos os valores observados positivos, o modelo acertou {recall :.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IT1Hvum2IIV",
        "outputId": "a57b1125-a2a2-4e45-ecd0-e6a1897b1756"
      },
      "id": "7IT1Hvum2IIV",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision : 0.5422773393461104\n",
            "O recall indica que de todos os valores observados positivos, o modelo acertou 54.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exemplo: Precision vs Recall**\n",
        "\n",
        "Imagine que ajustamos um modelo para detectar doenças\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1oUJCdnIpf4IqJvKA9IR3Re86TQK1p5S8\" width=\"700\" align=\"center\"/>\n",
        "    \n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "Gv4s9xsm-b4I"
      },
      "id": "Gv4s9xsm-b4I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "mWJkTkW42l-L"
      },
      "id": "mWJkTkW42l-L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **F-1 Score**\n",
        "\n",
        "Média harmônica de Precision e Recall:\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{F1 Score} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "\n",
        "O F1-Score é uma maneira de observar somente 1 métrica. É uma média harmônica entre o precision e o recall, que está muito mais próxima dos menores valores do que uma média aritmética simples. Ou seja, quando tem-se um F1-Score baixo, é um indicativo de que ou a precisão ou o recall está baixo.\n"
      ],
      "metadata": {
        "id": "JGqYV4NPz4Nk"
      },
      "id": "JGqYV4NPz4Nk"
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculando o F-1 Score com o Sklearn\n",
        "f1 = metrics.f1_score(y_test, y_pred_log)\n",
        "print (\"F-1 Score :\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_HrBTfE2Ivf",
        "outputId": "10c69134-c7dc-4ada-ce8f-c20bbdcde682"
      },
      "id": "Q_HrBTfE2Ivf",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-1 Score : 0.5574791742122419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mais informações sobre as métricas do sklearn, acesse: [Metrics Sklearn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)"
      ],
      "metadata": {
        "id": "TaqFMECHz4Jq"
      },
      "id": "TaqFMECHz4Jq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Classification Report**\n",
        "\n",
        "O **classification report** fornece uma visão geral das métricas principais como **precisão, recall, F1-score e suporte** para cada classe presente no conjunto de dados."
      ],
      "metadata": {
        "id": "QBFyM94oz4Gs"
      },
      "id": "QBFyM94oz4Gs"
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_test, y_pred_log))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqTIlGvU3DT7",
        "outputId": "eba128ee-cdea-4274-ea66-a68c082727ef"
      },
      "id": "iqTIlGvU3DT7",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.63      0.62      7804\n",
            "           1       0.57      0.54      0.56      7096\n",
            "\n",
            "    accuracy                           0.59     14900\n",
            "   macro avg       0.59      0.59      0.59     14900\n",
            "weighted avg       0.59      0.59      0.59     14900\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Precision de 57%:** De cada 100 previsões positivas, 43 estão erradas — o modelo está gerando muitos falsos positivos.\n",
        "\n",
        "**Recall de 54%:** O modelo está deixando de identificar quase metade dos casos positivos reais — isso representa falsos negativos.\n",
        "\n",
        "##### **Conclusão**\n",
        "\n",
        "- O modelo tem desempenho melhor para a classe 0 (negativa), o que pode indicar viés na predição, especialmente se a base estiver desbalanceada.\n",
        "\n",
        "- A baixa sensibilidade (recall) da classe 1 é preocupante, pois significa que o modelo não está conseguindo identificar com eficácia os casos mais relevantes.\n",
        "\n",
        "- Pode ser necessário:\n",
        "\n",
        "  - Trabalhar com técnicas de balanceamento (undersampling, oversampling, SMOTE).\n",
        "\n",
        "  - Ajustar o limiar de decisão (threshold).\n",
        "\n",
        "  - Experimentar modelos mais sensíveis ao recall.\n",
        "\n",
        "  - Usar métricas como F1-score ou AUC para priorizar ajustes."
      ],
      "metadata": {
        "id": "MpjGvwPa8DbI"
      },
      "id": "MpjGvwPa8DbI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### **Curva ROC**\n",
        "\n",
        "**ROC = Receiver Operating Characteristic**\n",
        "\n",
        "É uma curva que mostra a relação entre a taxa de verdadeiros positivos (Recall) e a taxa de falsos positivos, para diferentes limiares (thresholds) do modelo.\n",
        "\n",
        "Ao invés de avaliar apenas um limiar para o modelo, a curva ROC vai avaliar o comportamento global para todos os limiares possíveis. Ela responde a seguinte pergunta:\n",
        "\n",
        "> **“Quão bem meu modelo consegue distinguir entre classes positivas e negativas, independentemente de onde eu corto o threshold (limiar)?”**\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=17WXh34uzNtVM1v1Hz5WNk37xNNdivasP\" width=\"500\" align=\"center\"/>\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "BReG0Pnkz4D1"
      },
      "id": "BReG0Pnkz4D1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Curva ROC - Modelo Ruim**\n",
        "\n",
        "A diagonal representa um modelo que faz previsões completamente aleatórias.\n",
        "- Ela vai do ponto (0,0) até (1,1)\n",
        "- Para cada aumento no Falso Positivo Rate (FPR), há um aumento proporcional no Verdadeiro Positivo Rate (TPR).\n",
        "\n",
        "Ou seja:\n",
        "- O modelo não tem nenhuma habilidade real para distinguir entre classes positivas e negativas.\n",
        "- Ele “chuta” se é positivo ou negativo.\n",
        "\n",
        "\n",
        "> **“A diagonal da Curva ROC representa um modelo que não aprende nada: decisões aleatórias”**\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ddD11OEPbckaADcrMQEZgV2XlCcJq9xI\" width=\"500\" align=\"center\"/>\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "-fBNMMF6z4AK"
      },
      "id": "-fBNMMF6z4AK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Curva ROC - Modelo Bom**\n",
        "\n",
        "A curva acima da diagonal representa um modelo com capacidade discriminativa — ou seja, um modelo que consegue diferenciar positivamente as classes!\n",
        "\n",
        "- Cada ponto da curva mostra o desempenho para um determinado limiar.\n",
        "- A curva mostra como o modelo aumenta o Recall (TPR) ao custo de também aumentar um pouco o FPR (mas não tanto quanto o modelo aleatório).\n",
        "\n",
        "> **A curva ROC mede como o modelo equilibra Recall (positivos corretos) e Falsos Positivos, variando o threshold.**\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1cl0WGFEN3wo-cE0wKr_Xf92I2RE6R9lU\" width=\"500\" align=\"center\"/>\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "KCAOJgK5z377"
      },
      "id": "KCAOJgK5z377"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### **AUC - Área Sob a Curva ROC**\n",
        "\n",
        "A AUC é uma forma de resumir a Curva ROC em um único valor numérico.\n",
        "\n",
        "Assim como a Curva ROC mostra a relação entre a taxa de verdadeiros positivos (Recall) e a taxa de falsos positivos para todos os thresholds possíveis, o AUC responde à pergunta:\n",
        "\n",
        "> **Qual é a capacidade geral do meu modelo de separar positivos e negativos, independentemente do limiar escolhido?**\n",
        "\n",
        "\n",
        "\n",
        "| Valor do AUC | Interpretação                                                                 |\n",
        "|--------------|--------------------------------------------------------------------------------|\n",
        "| 1.0          | Separação perfeita: o modelo distingue todas as classes corretamente.          |\n",
        "| 0.5          | Sem poder discriminativo: equivalente a um modelo aleatório.                   |\n",
        "| < 0.5        | Pior que aleatório: o modelo inverte as classes.                               |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1gtFi8zXHLal1sqOvPcHLBBdzmdYIcwcX\" width=\"500\" align=\"center\"/>\n",
        "    \n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "2DoTL5g4z32m"
      },
      "id": "2DoTL5g4z32m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### **AUC vs Acurácia**\n",
        "\n",
        "Quando avaliamos modelos de classificação, é comum nos depararmos com várias métricas.\n",
        "\n",
        "\n",
        "Acurácia e AUC são duas das mais usadas, mas cada uma responde a uma pergunta diferente:\n",
        "- **Acurácia:** Mede o desempenho do modelo em um threshold fixo, calculando a proporção total de acertos.\n",
        "- **AUC:** Avalia a capacidade do modelo de separar classes considerando todos os possíveis limiares, mostrando o comportamento global.\n",
        "\n",
        "| Métrica    | O que mede?                                                                                     | Quando é mais útil?                                                                                 |\n",
        "|------------|--------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\n",
        "| Acurácia   | Proporção total de acertos (positivos e negativos) para um único threshold fixo.               | - Dados balanceados. <br> - Quando FP e FN têm peso semelhante.                                     |\n",
        "| AUC (ROC AUC) | Avalia a capacidade do modelo em todos os thresholds, medindo o trade-off entre Recall e Falso Positivo Rate. | - Quando queremos avaliar performance global. <br> - Dados desbalanceados. <br> - Quando não sabemos qual threshold será usado na prática. |\n"
      ],
      "metadata": {
        "id": "n5H6v_4Fz3ym"
      },
      "id": "n5H6v_4Fz3ym"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Quando usar cada métrica de classificação?**\n",
        "\n",
        "- **Acurácia:** Útil quando as classes estão balanceadas e você precisa de um indicador geral de desempenho.\n",
        "- **Precisão (Precision):** Importante quando o objetivo é minimizar os falsos positivos (ex.: detecção de fraudes).\n",
        "- **Recall (Sensibilidade):** Essencial quando os falsos negativos são mais críticos (ex.: diagnósticos de doenças).\n",
        "- **F1-Score:** Ideal para cenários com dados desbalanceados, quando é necessário equilibrar precisão e recall.\n",
        "- **AUC (ROC):** Ideal para avaliação global do modelo, comparação entre modelos, independente do limiar"
      ],
      "metadata": {
        "id": "WkV97Xkkz3vH"
      },
      "id": "WkV97Xkkz3vH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-_12jFEHz3iQ"
      },
      "id": "-_12jFEHz3iQ"
    },
    {
      "cell_type": "markdown",
      "id": "6cb62b0b",
      "metadata": {
        "id": "6cb62b0b"
      },
      "source": [
        "## **Validação de Modelos - Generalização**\n",
        "\n",
        "O objetivo principal de um modelo de machine learning não é apenas performar bem nos dados de treino, mas sim generalizar para dados nunca vistos.\n",
        "\n",
        "**Riscos Conhecidos:**\n",
        "- **Underfitting:** modelo com baixa capacidade, alto viés. Não aprende nem os padrões do treino.\n",
        "- **Overfitting:** modelo com alta capacidade, mas com alta variância. Aprende bem demais os dados de treino, mas não generaliza.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=15iW-oMZHq0uMZjggpTnL2-G5Sm1TaF18\" width=\"300\" align=\"center\"/>\n",
        "    \n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Particionamento dos Dados**\n",
        "\n",
        "A divisão dos dados é uma etapa essencial no desenvolvimento de modelos de Machine Learning. Ela assegura que o modelo seja treinado, validado e testado de forma adequada, prevenindo problemas como overfitting e garantindo uma avaliação realista da capacidade de generalização do modelo.\n",
        "\n",
        "Além dos métodos já trabalhados até o momento no curso, podemos utilizar outras técnicas como:\n",
        "* **Holdout (Divisão Simples)**\n",
        "* **Three-Way Split**\n",
        "* **Validação Cruzada**\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1kxNpvMWovyCTcb6C9xkSrpsoD9ECFQZD\" width=\"600\" align=\"center\"/>\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "GKyI3BjtAA-i"
      },
      "id": "GKyI3BjtAA-i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hold-out (Divisão Simples)**\n",
        "\n",
        "O Holdout é a forma mais simples — e também uma das mais utilizadas — para dividir dados em projetos de Machine Learning. Até este ponto do curso, essa foi a principal técnica que aplicamos para separar os dados durante o desenvolvimento dos modelos.\n",
        "Essa técnica separa a base de dados original em duas partes principais:\n",
        "- **Treinamento:** Onde o modelo aprende os padrões existentes nos dados.\n",
        "- **Teste:** Onde avaliamos o desempenho do modelo em dados que ele nunca viu.\n",
        "\n",
        "Geralmente, essa divisão é feita em proporções como 70% para treinamento e 30% para teste, ou 80/20.\n",
        "\n",
        "O objetivo é garantir que a avaliação do modelo seja feita de forma justa, simulando sua capacidade de generalizar para novos dados.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=13UK4E1WKMo26zbAVdZHs666iyQvVVgVc\" width=\"300\" align=\"center\"/>\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "fnmXsTNgAA8X"
      },
      "id": "fnmXsTNgAA8X"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Definir variáveis independente e dependentes\n",
        "x = df.drop(['Deixou a empresa'], axis=1)\n",
        "y = df['Deixou a empresa']\n",
        "\n",
        "\n",
        "#Definir bases de treino e teste, com 80% para treinamento do modelo\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y,\n",
        "                                                    test_size = 0.20,\n",
        "                                                    random_state = 42)"
      ],
      "metadata": {
        "id": "LAEpL6kZBN2r"
      },
      "id": "LAEpL6kZBN2r",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Three-Way Split**\n",
        "O Three-Way Split é uma extensão da divisão simples (Holdout), onde a base de dados é separada em três partes diferentes. Ele é muito usado quando precisamos ajustar hiperparâmetros do modelo sem utilizar o conjunto de teste para isso.\n",
        "\n",
        "**Treinamento:**\n",
        "Usado para ensinar o modelo (ex: 60-70% dos dados).\n",
        "\n",
        "\n",
        "**Validação:**\n",
        "Usado para ajustar hiperparâmetros e tomar decisões sobre o modelo (ex: 15-20%).\n",
        "\n",
        "\n",
        "**Teste:**\n",
        "Usado para avaliação final, simulando dados nunca vistos (ex: 15-20%).\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1NxH2opnN4W2ooxqhpfdV4i5IT0ekv0wz\" width=\"300\" align=\"center\"/>\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "UI1jJASvAA6C"
      },
      "id": "UI1jJASvAA6C"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir variáveis independentes e dependente\n",
        "X = df.drop(['Deixou a empresa'], axis=1)\n",
        "y = df['Deixou a empresa']\n",
        "\n",
        "# Passo 1: Separar 20% para teste\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y,\n",
        "                                                  test_size=0.20,\n",
        "                                                  random_state=42,\n",
        "                                                  stratify=y)  # para manter proporção das classes\n",
        "\n",
        "# Passo 2: Separar os 80% restantes em 60% treino e 20% validação (75% e 25% da parte restante)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp,\n",
        "                                                  test_size=0.25,  # 0.25 * 0.80 = 0.20 do total\n",
        "                                                  random_state=42,\n",
        "                                                  stratify=y_temp)"
      ],
      "metadata": {
        "id": "FuKGKJsoB022"
      },
      "id": "FuKGKJsoB022",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Particionamento dos Dados**\n",
        "\n",
        "Até agora, utilizamos técnicas como o Holdout e o Three-Way Split para dividir os dados. Porém, essas abordagens dependem de uma única divisão aleatória dos dados. O problema?\n",
        "\n",
        "> **Resultados podem variar bastante dependendo da sorte dessa divisão.**\n",
        "\n",
        "> ### Como reduzir esse risco e obter uma avaliação mais robusta?"
      ],
      "metadata": {
        "id": "XSRzXpeSAA3y"
      },
      "id": "XSRzXpeSAA3y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Validação Cruzada**\n",
        "\n",
        "A **Validação Cruzada (Cross-Validation)** é uma técnica utilizada para avaliar o desempenho de modelos de Machine Learning de forma mais confiável e robusta.\n",
        "\n",
        "Ao invés de depender de uma única divisão dos dados em treino e teste, como ocorre no Holdout, a validação cruzada divide os dados em várias partes (ou \"folds\") e realiza múltiplas rodadas de treinamento e teste.\n",
        "\n",
        "O modelo é treinado e testado várias vezes, garantindo que todas as observações sejam usadas tanto para treinar quanto para testar, mas nunca ao mesmo tempo.\n",
        "\n",
        "\n",
        "### **Técnicas**\n",
        "É importante destacar que existem diferentes variações dessa técnica, cada uma funciona melhor para situações específicas. A escolha da técnica correta pode fazer toda diferença na avaliação do modelo, dependendo do tipo de dados, do problema e do objetivo da análise. Abaixo temos uma tabela que resume as principais técnicas de validação cruzada, com uma descrição rápida e sobre quando utilizar cada uma.\n",
        "\n",
        "| Técnica               | O que faz?                                                                 | Quando usar                                                                                 |\n",
        "|-----------------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|\n",
        "| K-Fold Cross-Validation | Divide os dados em K partes; cada parte é usada uma vez como teste.        | Avaliação robusta, comum em datasets pequenos/médios.                                      |\n",
        "| Stratified K-Fold     | Variante do K-Fold que preserva a proporção das classes (em classificação). | Classes desbalanceadas.                                                                    |\n",
        "| Leave-One-Out (LOO)   | Cada observação é usada uma vez como teste; ideal para datasets muito pequenos. | Datasets extremamente pequenos, mas custo computacional alto.                              |\n",
        "| Group K-Fold          | Respeita grupos nos dados (ex: pacientes, hospitais).                       | Quando há dependência ou agrupamento nos dados.                                             |\n",
        "| Repeated K-Fold       | Repete várias vezes o processo de K-Fold com diferentes divisões aleatórias. | Para reduzir ainda mais a variância da avaliação.                                          |\n",
        "| Nested Cross-Validation | Validação cruzada aninhada para evitar viés na escolha de hiperparâmetros. | Quando ajustamos hiperparâmetros e avaliamos desempenho.                                   |\n",
        "| TimeSeriesSplit       | Usada para séries temporais, respeitando a ordem temporal dos dados.        | Modelos com dependência temporal (ex: previsão, séries).                                   |\n"
      ],
      "metadata": {
        "id": "Bbykx94vAAz9"
      },
      "id": "Bbykx94vAAz9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **\"Depois de rodar a Validação Cruzada, já posso usar o modelo para fazer predições?\"**\n",
        "\n",
        "#Não\n",
        "\n",
        "\n",
        "O que acontece na prática é:\n",
        "\n",
        "- A cada rodada do K-Fold:\n",
        "  - Um modelo diferente é treinado (com dados diferentes).\n",
        "  - O modelo é descartado após o fold.\n",
        "- **Não existe um modelo consolidado ao final da Validação Cruzada.**\n",
        "\n",
        "**Resultado da Validação Cruzada:**\n",
        "- Métricas médias (ex.: acurácia média, F1 médio).\n",
        "- Diagnóstico do comportamento geral do modelo.\n"
      ],
      "metadata": {
        "id": "ZaJ_Bu5ZAAwy"
      },
      "id": "ZaJ_Bu5ZAAwy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **K-fold Cross-Validation**\n",
        "\n",
        "O **K-Fold Cross-Validation** é uma das técnicas mais utilizadas para validar modelos de Machine Learning de forma confiável. Ao invés de dividir os dados apenas uma vez, como no Holdout, o K-Fold separa o conjunto de dados em K partes iguais e realiza várias rodadas de treino e teste.\n",
        "\n",
        "Dessa forma, todas as observações do dataset são usadas tanto para treinamento quanto para teste em momentos diferentes. Isso torna a avaliação do modelo mais completa e reduz a influência do acaso na divisão dos dados, resultando em uma estimativa mais estável do desempenho.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1V0cmbhzQZrQmMdmsZsrwznpEZ6kjoGi0\" width=\"600\" align=\"center\"/>\n",
        "    \n",
        "</div>\n",
        "\n",
        "\n",
        "O K-Fold Cross-Validation divide o conjunto de dados em K partes aproximadamente iguais. O modelo é então treinado e testado K vezes, o processo funciona assim:\n",
        "\n",
        "1. Em cada iteração, um **fold diferente é usado como teste**, enquanto os outros **K-1 folds são usados para treinamento**.\n",
        "2. Após cada rodada, calcula-se uma métrica de avaliação (como acurácia ou RMSE).\n",
        "3. Ao final, os **K resultados são combinados** através da média, resultando em uma estimativa mais confiável e estável do desempenho do modelo.\n",
        "\n",
        "Essa técnica garante que todas as observações sejam utilizadas tanto para treino quanto para teste, promovendo uma avaliação mais justa e robusta.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1gt1wLKFtz23WTEgxHV91MQ6CdgWwb1jh\" width=\"600\" align=\"center\"/>\n",
        "    \n",
        "</div>\n",
        "\n",
        "\n",
        "Apesar de ser uma das técnicas mais utilizadas para avaliação de modelos, o K-Fold Cross-Validation apresenta vantagens e limitações que precisam ser consideradas na prática.\n",
        "\n",
        "| Pontos Fortes                                   | Desvantagens                                              |\n",
        "|--------------------------------------------------|------------------------------------------------------------|\n",
        "| Utiliza todos os dados para treino e teste       | Custo computacional elevado (treina o modelo K vezes)      |\n",
        "| Avaliação mais robusta e confiável               | Pode gerar distribuição desigual em dados desbalanceados   |\n",
        "| Reduz dependência de uma única divisão           | Não indicado para dados temporais                          |\n",
        "| Flexível: ajustável para diferentes tamanhos de K|                                                            |\n",
        "\n",
        "\n",
        "> **IMPORTANTE: O K-Fold não serve para gerar o modelo final, mas para validar se o modelo que você está construindo é consistente e generaliza bem. Depois da validação, sempre treine o modelo definitivo com todos os dados disponíveis.**\n"
      ],
      "metadata": {
        "id": "uQ1AE-oyAAso"
      },
      "id": "uQ1AE-oyAAso"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Criar o objeto KFold com 5 folds, embaralhando os dados\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "# Criar o modelo (Árvore de Decisão)\n",
        "modelo_kf = DecisionTreeClassifier()\n",
        "\n",
        "# Avaliar o modelo usando validação cruzada com K-Fold\n",
        "scores = cross_val_score(modelo_kf, X_train, y_train, cv=kf)\n",
        "\n",
        "# Exibir os resultados\n",
        "print(f\"Acurácias por fold: {scores}\")\n",
        "print(f\"Média: {scores.mean():.2f}\")\n",
        "print(f\"Desvio padrão: {scores.std():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1MCUxPGEWzZ",
        "outputId": "e37d3a15-187a-4bb2-dc61-a893e0a0c41b"
      },
      "id": "L1MCUxPGEWzZ",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácias por fold: [0.66812081 0.65458613 0.67404922 0.65544244 0.66014096]\n",
            "Média: 0.66\n",
            "Desvio padrão: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold, cross_validate\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Criar o objeto KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "# Criar o modelo (Árvore de Decisão)\n",
        "modelo = DecisionTreeClassifier()\n",
        "\n",
        "# Avaliação com múltiplas métricas\n",
        "resultados = cross_validate(\n",
        "    modelo, X_train, y_train, cv=kf,\n",
        "    scoring=['accuracy', 'f1', 'precision', 'recall'],\n",
        "    return_train_score=False\n",
        ")\n",
        "\n",
        "# Exibir resultados\n",
        "print(\"Acurácia:\", resultados['test_accuracy'])\n",
        "print(\"F1-Score:\", resultados['test_f1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSWJ-R54FVMM",
        "outputId": "4083fee7-b12f-42f7-8e20-1aac5257b6a7"
      },
      "id": "MSWJ-R54FVMM",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: [0.66487696 0.65715884 0.67024609 0.65309319 0.65588992]\n",
            "F1-Score: [0.65187079 0.63911456 0.65325806 0.63539095 0.64265799]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stratified K-fold Cross-Validation**\n",
        "\n",
        "\n",
        "Quando lidamos com problemas de classificação, é comum encontrarmos base de dados com classes desbalanceadas (ex: 90% de uma classe, 10% de outra).\n",
        "\n",
        "Se aplicarmos o K-Fold tradicional, a divisão dos dados pode gerar folds onde essa proporção das classes não é preservada, o que pode distorcer a avaliação do modelo.\n",
        "\n",
        "\n",
        "**O Stratified K-Fold resolve esse problema**\n",
        "\n",
        "> Ele divide o conjunto de dados em K folds, mantendo aproximadamente a mesma proporção de classes em cada fold que existe no dataset original.\n",
        "\n",
        "Assim, cada rodada de treino e teste mantém a proporção original das classes no target, garantindo que a avaliação do modelo seja feita de forma equilibrada e consistente, especialmente em casos de dados desbalanceados."
      ],
      "metadata": {
        "id": "J0G6ziqZAAqA"
      },
      "id": "J0G6ziqZAAqA"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Criar Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Modelo\n",
        "modelo = DecisionTreeClassifier()\n",
        "\n",
        "# Avaliação\n",
        "scores = cross_val_score(modelo, X_train, y_train, cv=skf)\n",
        "print(f'Acurácias por fold: {scores}')\n",
        "print(f'Média: {scores.mean():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5LMdEthGV32",
        "outputId": "9baf0961-700d-4605-8beb-eb165e78bdbe"
      },
      "id": "E5LMdEthGV32",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácias por fold: [0.65324385 0.65604027 0.66409396 0.67143976 0.65734422]\n",
            "Média: 0.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Leave-on-Out (LOOCV)**\n",
        "\n",
        "O **Leave-One-Out Cross-Validation (LOOCV)** é uma variação extrema da validação cruzada. Ele segue o mesmo princípio do K-Fold, mas com um detalhe importante:\n",
        "\n",
        "**Cada fold contém exatamente uma observação.**\n",
        "\n",
        "Ou seja, o modelo é treinado com **n-1 observações** e testado com apenas 1 observação, repetindo esse processo para cada exemplo do dataset.\n",
        "\n",
        "Essa técnica é especialmente útil quando temos conjuntos de dados muito pequenos, já que permite aproveitar praticamente todos os dados para treinamento em cada iteração.\n",
        "\n",
        "\n",
        "Embora o Leave-One-Out Cross-Validation (LOOCV) siga a mesma ideia do K-Fold, ele possui características específicas que trazem impactos importantes na prática. Como qualquer técnica, o LOOCV oferece benefícios em alguns cenários, mas também apresenta limitações que precisam ser consideradas na escolha do método de validação.\n",
        "\n",
        "Os **principais pontos fortes e desvantagens** do LOOCV são:\n"
      ],
      "metadata": {
        "id": "QRixY77mAAm9"
      },
      "id": "QRixY77mAAm9"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Cria um objeto loocv\n",
        "loocv = LeaveOneOut()\n",
        "\n",
        "#Criar um modelo de Regressão Logística utilizando k-Fold\n",
        "modelo_loocv = LogisticRegression()\n",
        "\n",
        "#Definir a performance do modelo\n",
        "#scores = cross_val_score(modelo_loocv, X_train, y_train, cv=loocv)"
      ],
      "metadata": {
        "id": "N9g45kjLHCV4"
      },
      "id": "N9g45kjLHCV4",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ff87b44d",
      "metadata": {
        "id": "ff87b44d"
      },
      "source": [
        "---\n",
        "## Otimização de Hiperparâmetros\n",
        "\n",
        "Após definirmos nosso modelo e avaliarmos sua performance utilizando validação cruzada, surge uma pergunta essencial:\n",
        "\n",
        "> **Será que conseguimos melhorar ainda mais o desempenho do modelo?**\n",
        "\n",
        "Em muitos algoritmos, existem parâmetros que não são aprendidos diretamente durante o treinamento, mas que controlam o comportamento do modelo. Esses parâmetros são chamados de:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f92afc70",
      "metadata": {
        "id": "f92afc70"
      },
      "source": [
        "###**Hiperparâmetros**\n",
        "\n",
        "Diferente dos parâmetros aprendidos pelo modelo (como os coeficientes em uma regressão), os hiperparâmetros controlam o comportamento e a complexidade do modelo.\n",
        "\n",
        "Cada algoritmo possui hiperparâmetros específicos que influenciam diretamente sua performance. Escolher esses valores corretamente é um passo essencial para garantir um modelo eficiente e equilibrado. Podemos observar os principais hiperparâmetros de alguns modelos:\n",
        "\n",
        "\n",
        "| Modelo                   | Hiperparâmetros comuns                                          |\n",
        "|--------------------------|------------------------------------------------------------------|\n",
        "| Árvore de Decisão        | Profundidade máxima, número mínimo de amostras por folha        |\n",
        "| K-Nearest Neighbors (KNN)| Número de vizinhos (k), tipo de distância                       |\n",
        "| SVM                      | Parâmetros C e gamma                                            |\n",
        "| Random Forest            | Número de árvores, profundidade máxima                          |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Hiperparâmetros bem ajustados podem **melhorar significativamente o desempenho.**\n",
        "* Escolher valores arbitrários ou padrão nem sempre é ideal.\n",
        "* Ajuste manual é trabalhoso e sujeito a erro."
      ],
      "metadata": {
        "id": "z4pIFNs_IhCM"
      },
      "id": "z4pIFNs_IhCM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Otimização de Hiperparâmetros**\n",
        "\n",
        "> **Agora que temos nosso modelo treinado e validado, como sabemos se estamos usando os melhores hiperparâmetros?**\n",
        "\n",
        "---\n",
        "\n",
        "Agora que chegamos até aqui, não basta apenas definir bons hiperparâmetros — precisamos de uma forma mais sistemática para encontrar a melhor combinação. Para isso, utilizamos técnicas de Otimização de Hiperparâmetros, avaliando diferentes configurações com apoio da validação cruzada. As principais são:\n",
        "\n",
        "- **Grid Search**\n",
        "- **Random Search**\n",
        "\n",
        "Ambas têm o mesmo objetivo: **testar várias configurações e identificar aquela que oferece o melhor desempenho**, sempre validando de forma robusta com validação cruzada.\n",
        "\n",
        "O que muda entre elas é a forma como essas combinações são escolhidas e o custo computacional envolvido.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1OJn6rf5ux2XB_ttlTg2BxJIJSTXRuQCr\" width=\"600\" align=\"center\"/>\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "3jGCSR_oIqqj"
      },
      "id": "3jGCSR_oIqqj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Validação Cruzada e Otimização**\n",
        "\n",
        "Agora, imagine que além de testar diferentes combinações de hiperparâmetros, também podemos aplicar a validação cruzada dentro desse processo.\n",
        "\n",
        "Ou seja, para cada combinação de hiperparâmetros, o modelo não é avaliado apenas em uma divisão fixa dos dados, mas sim em várias divisões, garantindo que o desempenho daquela configuração seja consistente e não dependa da sorte de uma única partição.\n",
        "\n",
        "Essa é exatamente a proposta do `GridSearchCV` e do `RandomizedSearchCV`:\n",
        "\n",
        "> **Integrar de forma automática a validação cruzada ao ajuste dos hiperparâmetros.**\n",
        "\n",
        "Assim, conseguimos otimizar os hiperparâmetros e, ao mesmo tempo, validar a estabilidade do modelo de maneira robusta, tudo em um único processo.\n",
        "\n"
      ],
      "metadata": {
        "id": "tXXHo0oxJjX8"
      },
      "id": "tXXHo0oxJjX8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GRIDSEARCHCV: Otimização e Validação Cruzada**\n",
        "\n",
        "O **GridSearchCV** automatiza o processo de encontrar a melhor combinação de hiperparâmetros para um modelo, de forma sistemática e robusta.\n",
        "\n",
        "Como funciona:\n",
        "1. Define uma grade de valores para cada hiperparâmetro.\n",
        "2. Para **cada combinação possível**, o modelo é treinado e avaliado.\n",
        "3. A avaliação é feita utilizando **validação cruzada** em cada configuração, garantindo consistência.\n",
        "4. Retorna a combinação que apresenta o melhor desempenho médio nos folds.\n",
        "\n",
        "```\n",
        "# Hiperparâmetros a serem testados\n",
        "parametros = {\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "8aaP1YTzJyk1"
      },
      "id": "8aaP1YTzJyk1"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Hiperparâmetros a serem testados\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "\n",
        "# Criar o modelo\n",
        "modelo = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Definir K-Fold para validação cruzada\n",
        "cv = StratifiedKFold(n_splits=5,\n",
        "                     shuffle=True,\n",
        "                     random_state=42)\n",
        "\n",
        "# Configurar o GridSearchCV com validação cruzada\n",
        "grid = GridSearchCV(estimator=modelo,\n",
        "                    param_grid=param_grid,\n",
        "                    cv=cv,\n",
        "                    scoring='accuracy')\n",
        "\n",
        "# Treinar com validação cruzada para cada combinação de hiperparâmetros\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Resultados\n",
        "print(\"Melhores hiperparâmetros encontrados:\", grid.best_params_)\n",
        "print(f\"Melhor média de acurácia (validação cruzada): {grid.best_score_:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3DJkQMDKpuh",
        "outputId": "82a138f0-3a4c-404d-c9c4-e2b47dbfdf87"
      },
      "id": "d3DJkQMDKpuh",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores hiperparâmetros encontrados: {'max_depth': 7, 'min_samples_split': 10}\n",
            "Melhor média de acurácia (validação cruzada): 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Para cada combinação** de max_depth e min_samples_split, o modelo é avaliado em 5 folds diferentes (K-Fold).\n",
        "- O **GridSearchCV** cuida automaticamente de aplicar a validação cruzada em cada configuração.\n",
        "- Ao final, ele retorna a configuração com a **melhor média de desempenho nos folds**."
      ],
      "metadata": {
        "id": "gcKfl0QjLSPH"
      },
      "id": "gcKfl0QjLSPH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RANDOMIZEDSEARCHCV: Otimização e Validação Cruzada**\n",
        "\n",
        "O **RandomizedSearchCV** automatiza o ajuste dos hiperparâmetros, mas em vez de testar todas as combinações possíveis, ele seleciona combinações aleatórias dentro de um intervalo ou lista definida.\n",
        "\n",
        "Como funciona:\n",
        "1. Define intervalos ou listas de valores para os hiperparâmetros.\n",
        "2. Seleciona aleatoriamente um número fixo de combinações (definido pelo argumento n_iter).\n",
        "3. Para cada combinação sorteada, o modelo é treinado e avaliado.\n",
        "4. Usa validação cruzada internamente para avaliar cada configuração.\n",
        "5. Retorna a configuração com melhor desempenho médio.\n",
        "\n",
        "\n",
        "```\n",
        "# Hiperparâmetros a serem testados\n",
        "parametros = {\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'min_samples_split': randint(2, 11)\n",
        "}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "GdrVr72qLjH3"
      },
      "id": "GdrVr72qLjH3"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Definir o modelo\n",
        "modelo = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Definir os hiperparâmetros e os intervalos de busca\n",
        "param = {\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'min_samples_split': randint(2, 11)\n",
        "}\n",
        "\n",
        "# Definir K-Fold estratificado para validação cruzada\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configurar o RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=modelo,param_distributions=param,\n",
        "    n_iter=10,\n",
        "    cv=cv,scoring='accuracy',random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# Treinar com validação cruzada\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Resultados\n",
        "print(\"Melhores hiperparâmetros encontrados:\", random_search.best_params_)\n",
        "print(f\"Melhor média de acurácia (validação cruzada): {random_search.best_score_:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URtawIHoMEsT",
        "outputId": "f9150d60-ff73-4a79-9e2a-114c3b8740c7"
      },
      "id": "URtawIHoMEsT",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores hiperparâmetros encontrados: {'max_depth': 7, 'min_samples_split': 9}\n",
            "Melhor média de acurácia (validação cruzada): 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Resumindo**\n",
        "\n",
        "- Hiperparâmetros controlam o comportamento do modelo e influenciam diretamente seu desempenho.\n",
        "- Ajustá-los manualmente não é eficiente e pode levar a resultados inconsistentes.\n",
        "- Técnicas sistemáticas:\n",
        "  - **GridSearchCV**: Testa todas as combinações possíveis.\n",
        "  - **RandomizedSearchCV**: Testa combinações aleatórias.\n",
        "- Ambos integram Validação Cruzada, garantindo uma avaliação robusta.\n",
        "\n",
        "**Limitações:**\n",
        "- **Grid Search:** Alto custo computacional com muitos hiperparâmetros.\n",
        "- **Random Search:** Não garante explorar todo o espaço, depende do número de iterações.\n",
        "Ambos não aprendem com resultados anteriores (busca \"cega\").\n"
      ],
      "metadata": {
        "id": "cZg1mTZUMXLi"
      },
      "id": "cZg1mTZUMXLi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Referências**\n",
        "\n",
        "- Anderson, R. A., Sweeney, J. D. e Williams, T. A. [Estatística Aplicada à Administração e Economia](https://g.co/kgs/bjc2xdy). Editora Cengage. 4ª edição, 2019.\n",
        "- McKinney, W. [Python for Data Analysis: Data Wrangling with Pandas, NumPy & Jupyter](https://wesmckinney.com/book/). 3rd Edition. Released: August, 2022. Last Update: April 12, 2023.\n",
        "- VanderPlas, J. [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) - O'Reilly Media, Inc. Released November, 2016. Last Update:  May 5, 2023.\n",
        "- Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Edition). O'Reilly Media.\n",
        "- [Stats Models](https://www.statsmodels.org/stable/index.html) - Machine Learning in Python\n",
        "- [Scikit-learn](https://scikit-learn.org/stable/) - -Statistical models, hypothesis tests, and data exploration\n",
        "- **Scikit-learn – Model Selection**. https://scikit-learn.org/stable/model_selection.html\n",
        "- **Scikit-learn – Cross-validation:** evaluating estimator performance.  https://scikit-learn.org/stable/modules/cross_validation.html\n",
        "- **Scikit-learn – GridSearchCV**. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "- **Scikit-learn – RandomizedSearchCV**. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
        "- **Scikit-learn – GroupKFold.**  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html\n",
        "- **Scikit-learn – GroupShuffleSplit.**  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupShuffleSplit.html\n",
        "\n"
      ],
      "metadata": {
        "id": "SMnFsJ50PA0F"
      },
      "id": "SMnFsJ50PA0F"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}